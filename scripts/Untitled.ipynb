{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1855cec-0352-44f7-ba37-c10dd4d1881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "from dask.distributed import Client\n",
    "from tqdm import tqdm\n",
    "\n",
    "cli = Client()\n",
    "# try:\n",
    "#     ray.init()\n",
    "# except:\n",
    "#     pass\n",
    "# import modin.pandas as pd\n",
    "# from data_handling import get_info\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    StackingRegressor,\n",
    ")\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import (\n",
    "    ARDRegression,\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    "    LassoLars,\n",
    "    LassoLarsIC,\n",
    "    RANSACRegressor,\n",
    "    RidgeCV,\n",
    "    TweedieRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "# import ray\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 90\n",
    "pd.options.display.max_rows = 90\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_config(display=\"diagram\")\n",
    "dirty_results = defaultdict(dict)\n",
    "reference_metadata = defaultdict(dict)\n",
    "clean_results = defaultdict(dict)\n",
    "\n",
    "patch_sklearn()\n",
    "iter_ = 10000\n",
    "tol = 0.000001\n",
    "\n",
    "\n",
    "def gen_stack():\n",
    "    # Category Selector\n",
    "    # cat_selector = make_column_selector(dtype_exclude=np.float32)\n",
    "    # Number Selector\n",
    "    # cat_scaler = OneHotEncoder(sparse=True)\n",
    "    # linear_prep = ColumnTransformer(\n",
    "    #     transformers=[(\"num\", numeric_scaler, numerical_selector)]\n",
    "    # )\n",
    "    # category_transformer\n",
    "    # Feature Selector\n",
    "    numerical_selector = make_column_selector(dtype_exclude=np.uint8)\n",
    "    sel = SelectFromModel(estimator=ElasticNet(precompute=True), threshold=\"median\")\n",
    "    numeric_scaler = StandardScaler()\n",
    "    tree_prep = ColumnTransformer(transformers=[(\"num\", numeric_scaler, numerical_selector)])\n",
    "    lasso_linear_prep = ColumnTransformer(transformers=[(\"num\", numeric_scaler, numerical_selector)])\n",
    "    modis = [\n",
    "        make_pipeline(lasso_linear_prep, sel, LassoLarsIC(normalize=False, precompute=True, criterion=\"bic\")),\n",
    "        make_pipeline(lasso_linear_prep, sel, ARDRegression(n_iter=1000, compute_score=True, tol=tol)),\n",
    "        make_pipeline(\n",
    "            lasso_linear_prep, sel, BayesianRidge(lambda_init=0.001, n_iter=iter_, tol=tol, compute_score=True)\n",
    "        ),\n",
    "        make_pipeline(lasso_linear_prep, sel, Lasso(precompute=True, max_iter=iter_, tol=tol, selection=\"cyclic\")),\n",
    "        make_pipeline(lasso_linear_prep, sel, LassoLars(precompute=True, max_iter=iter_)),\n",
    "        make_pipeline(lasso_linear_prep, sel, TweedieRegressor(power=0)),\n",
    "        make_pipeline(\n",
    "            lasso_linear_prep,\n",
    "            sel,\n",
    "            RANSACRegressor(\n",
    "                min_samples=500,\n",
    "                base_estimator=LassoLarsIC(normalize=False, precompute=True, criterion=\"aic\"),\n",
    "                max_trials=10000,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            lasso_linear_prep,\n",
    "            sel,\n",
    "            ElasticNet(\n",
    "                precompute=True,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, HistGradientBoostingRegressor(max_iter=1000, max_depth=500)),\n",
    "        make_pipeline(tree_prep, sel, GradientBoostingRegressor(random_state=0, max_depth=30)),\n",
    "        make_pipeline(tree_prep, sel, DecisionTreeRegressor()),\n",
    "        make_pipeline(\n",
    "            tree_prep,\n",
    "            sel,\n",
    "            ExtraTreesRegressor(n_jobs=-1),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, AdaBoostRegressor(base_estimator=Lasso(precompute=True))),\n",
    "    ]\n",
    "    stacked_estimators = []\n",
    "    for q in modis:\n",
    "        estimator_name = q[2].__class__.__name__\n",
    "        stacked_estimators.append((estimator_name, q))\n",
    "    learning_stack = StackingRegressor(estimators=stacked_estimators, cv=3, n_jobs=-1, final_estimator=RidgeCV())\n",
    "    return learning_stack\n",
    "\n",
    "\n",
    "def save_pipeline(c, p):\n",
    "    with open(f\"stacking_models/stack_{c}.pkl\", \"wb+\") as file_output:\n",
    "        pickle.dump(p, file_output)\n",
    "\n",
    "\n",
    "def get_data_feed(c, x_y):\n",
    "    training_features = x_y.drop([c], axis=1)\n",
    "    y = x_y[c]\n",
    "    x_temp_train, x_temp_test, y_temp_train, y_temp_train = train_test_split(\n",
    "        training_features, y, test_size=0.2, random_state=0\n",
    "    )\n",
    "    return x_temp_train, x_temp_test, y_temp_train, y_temp_train\n",
    "\n",
    "\n",
    "training_targets = [\n",
    "    \"F_1_0\",\n",
    "    \"F_1_1\",\n",
    "    \"F_1_2\",\n",
    "    \"F_1_3\",\n",
    "    \"F_1_4\",\n",
    "    \"F_1_5\",\n",
    "    \"F_1_6\",\n",
    "    \"F_1_7\",\n",
    "    \"F_1_8\",\n",
    "    \"F_1_9\",\n",
    "    \"F_1_10\",\n",
    "    \"F_1_11\",\n",
    "    \"F_1_12\",\n",
    "    \"F_1_13\",\n",
    "    \"F_1_14\",\n",
    "    \"F_3_0\",\n",
    "    \"F_3_1\",\n",
    "    \"F_3_2\",\n",
    "    \"F_3_3\",\n",
    "    \"F_3_4\",\n",
    "    \"F_3_5\",\n",
    "    \"F_3_6\",\n",
    "    \"F_3_7\",\n",
    "    \"F_3_8\",\n",
    "    \"F_3_9\",\n",
    "    \"F_3_10\",\n",
    "    \"F_3_11\",\n",
    "    \"F_3_12\",\n",
    "    \"F_3_13\",\n",
    "    \"F_3_14\",\n",
    "    \"F_3_15\",\n",
    "    \"F_3_16\",\n",
    "    \"F_3_17\",\n",
    "    \"F_3_18\",\n",
    "    \"F_3_19\",\n",
    "    \"F_3_20\",\n",
    "    \"F_3_21\",\n",
    "    \"F_3_22\",\n",
    "    \"F_3_23\",\n",
    "    \"F_3_24\",\n",
    "    \"F_4_0\",\n",
    "    \"F_4_1\",\n",
    "    \"F_4_2\",\n",
    "    \"F_4_3\",\n",
    "    \"F_4_4\",\n",
    "    \"F_4_5\",\n",
    "    \"F_4_6\",\n",
    "    \"F_4_7\",\n",
    "    \"F_4_8\",\n",
    "    \"F_4_9\",\n",
    "    \"F_4_10\",\n",
    "    \"F_4_11\",\n",
    "    \"F_4_12\",\n",
    "    \"F_4_13\",\n",
    "    \"F_4_14\",\n",
    "]\n",
    "\n",
    "\n",
    "def gen_sparse_data(dpkl) -> pd.DataFrame:\n",
    "    cats = [x for x in dpkl.columns if \"F_2\" in x]\n",
    "    # for c in cats:\n",
    "    #     dpkl = feature_one_hot(c)\n",
    "    spar = pd.DataFrame()\n",
    "    cat_pd = dpkl.loc[:, cats].copy()\n",
    "\n",
    "    for fi in cats:\n",
    "        new_n = fi.replace(\"_\", \"\") + \"_\"\n",
    "        f_one_hot = pd.get_dummies(dpkl[fi], prefix=new_n)\n",
    "        spar_ = f_one_hot.astype(pd.SparseDtype(np.uint8, fill_value=0))\n",
    "        for c in spar_.columns:\n",
    "            spar[c] = spar_[c]\n",
    "    dpkl = dpkl.drop(cats, axis=1)\n",
    "    for c in spar.columns:\n",
    "        dpkl[c] = spar[c]\n",
    "    return dpkl\n",
    "\n",
    "\n",
    "def run(dataset_db):\n",
    "    trd = dataset_db[dataset_db.missing_cols == 0].copy()\n",
    "    x_y = trd.drop([\"missing_cols\"], axis=1)\n",
    "    for cl in tqdm(training_targets):\n",
    "        training_features = x_y.drop([cl], axis=1)\n",
    "        y = x_y[cl]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(training_features, y, test_size=0.2, random_state=0)\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        with parallel_backend(\"dask\"):\n",
    "            new_stack = gen_stack()\n",
    "            new_stack.fit(x_train, y_train)\n",
    "        yp = new_stack.predict(x_test)\n",
    "        with open(\"results_mod\", \"a\") as fp:\n",
    "            fp.write(f\"{cl} : {mean_squared_error(yp, y_test)}\\n\\n\")\n",
    "        print(mean_squared_error(yp, y_test))\n",
    "        save_pipeline(cl, new_stack)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with parallel_backend(\"multiprocessing\", n_jobs=-1):\n",
    "    dataset = pd.read_pickle(\"cooked.pkl\")\n",
    "    run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29047d58-20cf-433c-b09d-8b1f04223553",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"data.pkl\")\n",
    "\n",
    "# sparse = pd.read_pickle(\"cooked_sparsely.pkl\")1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a8e5f0-2035-427e-b5cd-2ded807e5cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec5455-c85b-4372-b329-01b332c8d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"F_4_11\"\n",
    "# ray.init()\n",
    "# import ray\n",
    "\n",
    "dataset_db = dataset.copy()\n",
    "trd = dataset_db[dataset_db.missing_cols == 0].copy()\n",
    "x_y = trd.drop([\"missing_cols\"], axis=1)\n",
    "# with parallel_backend(\"mulitprocessing\", n_jobs=-1):\n",
    "# for cl in training_targets:\n",
    "gc.collect()\n",
    "training_features = x_y.drop([c], axis=1)\n",
    "y = x_y[c]\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_features, y, test_size=0.2, random_state=0)\n",
    "# x_train, x_test, y_train, y_test =\n",
    "new_stack = gen_stack()\n",
    "y_pred = new_stack.fit(x_train, y_train).predict(x_test)\n",
    "# print(mean_squared_error(y_pred, y_test))\n",
    "# save_pipeline(c, new_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7169cd06-25ab-40d7-b912-51942e8d142f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72955,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb648ca-6de1-49e1-badb-d58387f15e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for de in sparse.select_dtypes(include=\"int64\").columns:\n",
    "    sparse[de] = sparse[de].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d66810c-f13e-41c0-8052-ca2dfc1fe2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'modin.pandas.dataframe.DataFrame'>\n",
      "Int64Index: 1000000 entries, 0 to 999999\n",
      "Columns: 424 entries, F_1_0 to F224__17\n",
      "dtypes: Sparse[uint8, 0](368), float32(55), uint8(1)\n",
      "memory usage: 337.6 MB\n"
     ]
    }
   ],
   "source": [
    "sparse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c937895-39db-4eb4-992a-6382cae37f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'modin.pandas.dataframe.DataFrame'>\n",
      "Int64Index: 1000000 entries, 0 to 999999\n",
      "Columns: 424 entries, F_1_0 to F224__17\n",
      "dtypes: Sparse[uint8, 0](368), float32(55), int64(1)\n",
      "memory usage: 344.3 MB\n"
     ]
    }
   ],
   "source": [
    "sparse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb4facd-09ee-4815-8377-6f7f5fcf33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "cli = Client()\n",
    "with parallel_backend(\"dask\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952917bf-c7cd-44f0-9efd-90f9e8619ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5151ccb-3551-4a66-a213-f0c4914b1489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645663b-86af-4475-9cd6-94a45989a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3a18d-d004-4a73-bcbc-8260fb473e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e85b5c-e059-4d45-a625-c316e5c6a959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01eab0f-2a81-45c1-8bdb-8736a1e802db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dc3ae-341a-4ed9-b11f-5e8083e24531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     with parallel_backend(\"multiprocessing\", n_jobs=-1):\n",
    "\n",
    "#         with open(\"data.pkl\", \"rb\") as fp:\n",
    "#             dataset = pickle.load(fp)\n",
    "# run(dataset)\n",
    "# get_info(dataset)\n",
    "# print(f\"dense_type: {dataset.memory_usage(index=True, deep=True).sum() / 10 ** 6}MB\")\n",
    "# sparse_data = gen_sparse_data(dataset)\n",
    "# print(sparse_data.dtypes.value_counts())\n",
    "# print(f\"sparse_type: {sparse_data.memory_usage(index=True, deep=True).sum() / 10 ** 6}MB\")\n",
    "# sparse_data_np: np.ndarray = sparse_data.to_numpy()\n",
    "# print(f\"numpy_type: {sparse_data_np.nbytes / 10 ** 6}MB\")\n",
    "\n",
    "# start = 3\n",
    "# if start == 3:\n",
    "#     for cl in trgs:\n",
    "#         # with dpctl.device_context(\"opencl:gpu\"):\n",
    "#         with parallel_backend(\"threading\", n_jobs=-1):\n",
    "#             gc.collect()\n",
    "#             X_train, X_test, y_train, y_test = get_data_feed(cl)\n",
    "#             new_stack = gen_stack()\n",
    "#             gc.collect()\n",
    "#\n",
    "#             new_stack.fit(X_train, y_train)\n",
    "#             yp = new_stack.predict(X_test)\n",
    "#             save_pipeline(cl, new_stack)\n",
    "#             print(mean_squared_error(yp, y_test))\n",
    "#         break\n",
    "#\n",
    "# def feature_one_hot(f, df):\n",
    "#     f_one_hot = pd.get_dummies(df[f], prefix=str(f))\n",
    "#     df = pd.concat([df, f_one_hot], axis=1)\n",
    "#     df.drop([f], axis=1, inplace=True)\n",
    "#     return df.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
