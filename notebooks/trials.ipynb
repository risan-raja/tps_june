{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbcc687-d2b3-4181-9764-75a26a16ce8a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391d7cc-4b03-4061-bdf8-0759239d5f81",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import dpctl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    StackingRegressor,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    SelectPercentile,\n",
    "    SequentialFeatureSelector,\n",
    "    mutual_info_regression,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    ARDRegression,\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    GammaRegressor,\n",
    "    Lasso,\n",
    "    LassoLars,\n",
    "    LassoLarsIC,\n",
    "    LinearRegression,\n",
    "    MultiTaskElasticNet,\n",
    "    MultiTaskLasso,\n",
    "    PassiveAggressiveRegressor,\n",
    "    PoissonRegressor,\n",
    "    QuantileRegressor,\n",
    "    RANSACRegressor,\n",
    "    RidgeCV,\n",
    "    SGDRegressor,\n",
    "    TheilSenRegressor,\n",
    "    TweedieRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, PowerTransformer, QuantileTransformer, StandardScaler\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearnex.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRFRegressor as xrb\n",
    "\n",
    "pd.options.display.max_columns = 90\n",
    "pd.options.display.max_rows = 90\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "\n",
    "# def conn():\n",
    "#     with open(\"data_np.pkl\", \"rb\") as fp:\n",
    "#         dpkl = pickle.load(fp)\n",
    "#     return dpkl\n",
    "\n",
    "\n",
    "# X__ = conn()\n",
    "\n",
    "\n",
    "# def data_n_miss(n):\n",
    "#     # data with x features simultaneously missing\n",
    "#     # cdef np.ndarray t, idx\n",
    "#     t = X__[:, 80]\n",
    "#     idx = np.where(t == n)[0]\n",
    "#     return X__[idx, :80]\n",
    "\n",
    "\n",
    "# ms = data_n_miss(1)\n",
    "# dcl = data_n_miss(0)\n",
    "dirty_results = defaultdict(dict)\n",
    "reference_metadata = defaultdict(dict)\n",
    "clean_results = defaultdict(dict)\n",
    "\n",
    "\n",
    "patch_sklearn()\n",
    "iter_ = 10000\n",
    "tol = 0.000001\n",
    "\n",
    "# estimators = [\n",
    "#     LassoLarsIC(normalize=False, precompute=True, criterion=\"bic\"),\n",
    "#     # # ARDRegression(n_iter=1000,compute_score=True,tol=tol),\n",
    "#     BayesianRidge(lambda_init=0.001, n_iter=iter_, tol=tol, compute_score=True),\n",
    "#     Lasso(precompute=True, max_iter=iter_, tol=tol, selection=\"cyclic\"),\n",
    "#     LassoLars(precompute=True, max_iter=iter_),\n",
    "#     TweedieRegressor(power=0),\n",
    "#     RANSACRegressor(\n",
    "#         min_samples=500, base_estimator=LassoLarsIC(normalize=False, precompute=True, criterion=\"aic\"), max_trials=10000\n",
    "#     ),\n",
    "#     ElasticNet(\n",
    "#         precompute=True,\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "\n",
    "# def feat_n_miss():\n",
    "#     # Col Index of incomplete features\n",
    "#     # cdef np.ndarray feed, feat_idx\n",
    "#     global ms\n",
    "#     feed: np.ndarray = np.isnan(ms).sum(axis=0)\n",
    "#     feat_idx = np.where(feed > 0)[0]\n",
    "#     return feat_idx\n",
    "\n",
    "\n",
    "# def feat_x_nan_idx(feature, cc):\n",
    "#     # cc data_n_miss\n",
    "#     # return the indices of data missing \"x\" feature\n",
    "#     cl_ = cc[:, feature]\n",
    "#     return np.where(np.isnan(cl_))[0]\n",
    "\n",
    "\n",
    "# # # clean data for reference estimation\n",
    "# def get_clean_data(target_f, dirty_f_idx, size=0.5):\n",
    "#     global dcl\n",
    "#     clash = np.array([target_f, dirty_f_idx])\n",
    "#     d_features = np.setdiff1d(np.arange(80), clash)\n",
    "#     test_f = dcl[:, d_features]\n",
    "#     test_t = dcl[:, target_f]\n",
    "#     xt, xst, yt, yst = train_test_split(test_f, test_t, test_size=size, random_state=0, shuffle=True)\n",
    "#     return xt, yt\n",
    "\n",
    "\n",
    "# def gen_preprocessor():\n",
    "#     preprocessor = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"scaling\", StandardScaler()),\n",
    "#             # (\n",
    "#             #     \"f_select\",\n",
    "#             #     SequentialFeatureSelector(\n",
    "#             #         n_features_to_select=0.6,\n",
    "#             #         cv=3,\n",
    "#             #         estimator=ElasticNet(precompute=True),\n",
    "#             #         direction=\"forward\",\n",
    "#             #         n_jobs=-1,\n",
    "#             #     ),\n",
    "#             # ),\n",
    "#             (\"f_select\", Select(mutual_info_regression, percentile=50)),\n",
    "#         ],\n",
    "#         memory=\"learning/\",\n",
    "#     )\n",
    "#     return preprocessor\n",
    "\n",
    "\n",
    "# # Prepopulate Learning benchmark with complete data\n",
    "# # Testing QuantileTransformer\n",
    "# def clean_learning(target_f, size=0.7, exx=estimators.copy()):\n",
    "#     global clean_results, dcl, reference_metadata\n",
    "#     d_features = np.setdiff1d(np.arange(80), np.array(target_f))\n",
    "#     xf = dcl[:, d_features]\n",
    "#     yf = dcl[:, target_f]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(xf, yf, test_size=size, random_state=0, shuffle=True)\n",
    "#     reference_metadata[target_f] = defaultdict(dict)\n",
    "#     prep = gen_preprocessor()\n",
    "#     print(\"+++++++++++++++++++++++++++++ PREPARING DATA +++++++++++++++++++++++++++++\")\n",
    "#     with dpctl.device_context(\"gpu\"):\n",
    "#         with parallel_backend(\"loky\"):\n",
    "#             X_train_t = prep.fit_transform(X_train, y_train)\n",
    "#     reference_metadata[target_f][\"prep\"] = prep\n",
    "#     print(\"+++++++++++++++++++++++++++++ PREPARING DATA COMPLETE +++++++++++++++++++++++++++++\")\n",
    "#     # reference_metadata[target_f][\"models\"] = defaultdict(dict)\n",
    "#     models_data_hub = reference_metadata[target_f][\"models\"]\n",
    "#     models_data_hub = {}\n",
    "#     with parallel_backend(\"multiprocessing\"):\n",
    "#         for est in exx:\n",
    "#             est_name = est.__class__.__name__\n",
    "#             # e = make_pipeline(prep, est)\n",
    "#             est.fit(X_train_t, y_train)\n",
    "#             X_test_t = prep.transform(X_test)\n",
    "#             y_pred = est.predict(X_test_t)\n",
    "#             safer = gc.collect()\n",
    "#             clean_results[target_f][est_name] = mean_squared_error(y_pred, y_test)\n",
    "#             safer = gc.collect()\n",
    "#             models_data_hub[est_name] = est\n",
    "\n",
    "\n",
    "# def dirty_df(target_f, dirty_f_idx, ms=ms):\n",
    "#     # dirty_f_idx.append(target_f)\n",
    "#     clash = np.array([target_f, dirty_f_idx])\n",
    "#     # cdef np.ndarray clash\n",
    "#     d_features = np.setdiff1d(np.arange(80), clash)\n",
    "#     dirty_f_training_data_idx = feat_x_nan_idx(dirty_f_idx, ms)\n",
    "#     ark = ms[dirty_f_training_data_idx, :]\n",
    "#     ark = ark[:, d_features]\n",
    "#     ark_target = ms[dirty_f_training_data_idx, target_f]\n",
    "#     return ark, ark_target\n",
    "\n",
    "\n",
    "# def roll(target_f, dirty_f_idx):\n",
    "#     global estimators\n",
    "#     dirty_results[target_f][dirty_f_idx] = {}\n",
    "#     x_train, y_train = dirty_df(target_f, dirty_f_idx)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(xf, yf, test_size=size, random_state=0, shuffle=True)\n",
    "#     safer = gc.collect()\n",
    "#     # workflow = make_pipeline((), estimator)\n",
    "#     xtest, ytest = get_clean_data(target_f, dirty_f_idx)\n",
    "#     estimators_ = estimators.copy()\n",
    "#     with dpctl.device_context(\"gpu\"):\n",
    "\n",
    "#         for est in estimators_             with parallel_backend(\"multiprocessing\", n_jobs=-1):\n",
    "#                 workflow = make_pipeline(\n",
    "#                     PowerTransformer(),\n",
    "#                     SequentialFeatureSelector(\n",
    "#                         n_features_to_select=0.6,\n",
    "#                         cv=5,\n",
    "#                         estimator=ElasticNet(precompute=True),\n",
    "#                         direction=\"forward\",\n",
    "#                         n_jobs=-1,\n",
    "#                     ),\n",
    "#                     # SelectPercentile(mutual_info_regression,percentile=50),\n",
    "#                     est,\n",
    "#                     memory=\"learning/\",\n",
    "#                 )\n",
    "#                 # workflow = est\n",
    "#                 workflow.fit(x_train, y_train)\n",
    "#                 y_pred = workflow.predict(xtest)\n",
    "#                 safer = gc.collect()\n",
    "#                 dirty_results[target_f][dirty_f_idx][est.__class__.__name__] = mean_squared_error(y_pred, ytest)\n",
    "#                 # print(workflow.__class__.__name__,\":\",mean_squared_error(y_pred, ytest))\n",
    "\n",
    "\n",
    "def gen_stack():\n",
    "    # Category Selector\n",
    "    cat_selector = make_column_selector(dtype_exclude=np.float32)\n",
    "    # Number Selector\n",
    "    numerical_selector = make_column_selector(dtype_exclude=np.uint8)\n",
    "    # category_transformer =\n",
    "    # Feature Selector\n",
    "    sel = SelectFromModel(estimator=ElasticNet(precompute=True), threshold=\"median\")\n",
    "    numeric_scaler = StandardScaler()\n",
    "    cat_scaler = OneHotEncoder(sparse=True)\n",
    "    linear_prep = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_scaler, numerical_selector), (\"categ\", cat_scaler, cat_selector)]\n",
    "    )\n",
    "    tree_prep = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_scaler, numerical_selector), (\"categ\", OrdinalEncoder, cat_selector)]\n",
    "    )\n",
    "    lasso_linear_prep = ColumnTransformer(transformers=[(\"num\", numeric_scaler, numerical_selector)])\n",
    "    modis = [\n",
    "        make_pipeline(lasso_linear_prep, sel, LassoLarsIC(normalize=False, precompute=True, criterion=\"bic\")),\n",
    "        make_pipeline(lasso_linear_prep, sel, ARDRegression(n_iter=1000, compute_score=True, tol=tol)),\n",
    "        make_pipeline(linear_prep, sel, BayesianRidge(lambda_init=0.001, n_iter=iter_, tol=tol, compute_score=True)),\n",
    "        make_pipeline(linear_prep, sel, Lasso(precompute=True, max_iter=iter_, tol=tol, selection=\"cyclic\")),\n",
    "        make_pipeline(linear_prep, sel, LassoLars(precompute=True, max_iter=iter_)),\n",
    "        make_pipeline(linear_prep, sel, TweedieRegressor(power=0)),\n",
    "        make_pipeline(\n",
    "            linear_prep,\n",
    "            sel,\n",
    "            RANSACRegressor(\n",
    "                min_samples=500,\n",
    "                base_estimator=LassoLarsIC(normalize=False, precompute=True, criterion=\"aic\"),\n",
    "                max_trials=10000,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            linear_prep,\n",
    "            sel,\n",
    "            ElasticNet(\n",
    "                precompute=True,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, HistGradientBoostingRegressor(max_iter=1000, max_depth=500)),\n",
    "        make_pipeline(tree_prep, sel, GradientBoostingRegressor(random_state=0, max_depth=30)),\n",
    "        make_pipeline(tree_prep, sel, DecisionTreeRegressor()),\n",
    "        make_pipeline(\n",
    "            tree_prep,\n",
    "            sel,\n",
    "            ExtraTreesRegressor(n_jobs=-1),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, AdaBoostRegressor(base_estimator=Lasso(precompute=True))),\n",
    "    ]\n",
    "    stacked_estimators = []\n",
    "    for q in modis:\n",
    "        ename = q[2].__class__.__name__\n",
    "        stacked_estimators.append((ename, q))\n",
    "    learning_stack = StackingRegressor(estimators=stacked_estimators, cv=3, n_jobs=-1, final_estimator=RidgeCV())\n",
    "    return learning_stack\n",
    "\n",
    "\n",
    "with open(\"data.pkl\", \"rb\") as fp:\n",
    "    df = pickle.load(fp)\n",
    "\n",
    "trd = df[df.missing_cols == 0].copy()\n",
    "X_y = trd.drop([\"missing_cols\"], axis=1)\n",
    "\n",
    "\n",
    "def save_pipeline(c, p):\n",
    "    with open(f\"stacking_models/stack_{c}.pkl\", \"wb+\") as fp:\n",
    "        pickle.dump(p, fp)\n",
    "\n",
    "\n",
    "def get_data_feed(c):\n",
    "    X = X_y.drop([c], axis=1)\n",
    "    y = X_y[c]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# if\n",
    "#\n",
    "trgs = [\n",
    "    \"F_1_0\",\n",
    "    \"F_1_1\",\n",
    "    \"F_1_2\",\n",
    "    \"F_1_3\",\n",
    "    \"F_1_4\",\n",
    "    \"F_1_5\",\n",
    "    \"F_1_6\",\n",
    "    \"F_1_7\",\n",
    "    \"F_1_8\",\n",
    "    \"F_1_9\",\n",
    "    \"F_1_10\",\n",
    "    \"F_1_11\",\n",
    "    \"F_1_12\",\n",
    "    \"F_1_13\",\n",
    "    \"F_1_14\",\n",
    "    \"F_3_0\",\n",
    "    \"F_3_1\",\n",
    "    \"F_3_2\",\n",
    "    \"F_3_3\",\n",
    "    \"F_3_4\",\n",
    "    \"F_3_5\",\n",
    "    \"F_3_6\",\n",
    "    \"F_3_7\",\n",
    "    \"F_3_8\",\n",
    "    \"F_3_9\",\n",
    "    \"F_3_10\",\n",
    "    \"F_3_11\",\n",
    "    \"F_3_12\",\n",
    "    \"F_3_13\",\n",
    "    \"F_3_14\",\n",
    "    \"F_3_15\",\n",
    "    \"F_3_16\",\n",
    "    \"F_3_17\",\n",
    "    \"F_3_18\",\n",
    "    \"F_3_19\",\n",
    "    \"F_3_20\",\n",
    "    \"F_3_21\",\n",
    "    \"F_3_22\",\n",
    "    \"F_3_23\",\n",
    "    \"F_3_24\",\n",
    "    \"F_4_0\",\n",
    "    \"F_4_1\",\n",
    "    \"F_4_2\",\n",
    "    \"F_4_3\",\n",
    "    \"F_4_4\",\n",
    "    \"F_4_5\",\n",
    "    \"F_4_6\",\n",
    "    \"F_4_7\",\n",
    "    \"F_4_8\",\n",
    "    \"F_4_9\",\n",
    "    \"F_4_10\",\n",
    "    \"F_4_11\",\n",
    "    \"F_4_12\",\n",
    "    \"F_4_13\",\n",
    "    \"F_4_14\",\n",
    "]\n",
    "\n",
    "start = 3\n",
    "if start == 3:\n",
    "    for cl in trgs:\n",
    "        # with dpctl.device_context(\"opencl:gpu\"):\n",
    "        with parallel_backend(\"threading\", n_jobs=1):\n",
    "            gc.collect()\n",
    "            X_train, X_test, y_train, y_test = get_data_feed(cl)\n",
    "            new_stack = gen_stack()\n",
    "            gc.collect()\n",
    "\n",
    "            new_stack.fit(X_train, y_train)\n",
    "            yp = new_stack.predict(X_test)\n",
    "            save_pipeline(cl, new_stack)\n",
    "            print(mean_squared_error(yp, y_test))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8d93a-b874-4cd0-89d3-327e7630c20b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\tLassoLarsIC \tBayesianRidge \tLasso \tLassoLars \tTweedieRegressor \tRANSACRegressor \tElasticNet\n",
    "0 \t1.003801   \t1.003801 \t1.003801 \t1.003801 \t1.003801 \t1.004282 \t1.003801\n",
    "1 \t1.000459 \t1.000494 \t1.000459 \t1.000459 \t1.000464 \t1.002849 \t1.000459\n",
    "2 \t0.998591 \t0.998694 \t0.998591 \t0.998591 \t0.998607 \t0.999959 \t0.998591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623e1f7-3f32-4b0a-a49d-8e7e5945097f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_f = 0\n",
    "if target_f == 9899:\n",
    "    d_features = np.setdiff1d(np.arange(80), np.array(target_f))\n",
    "    xf = dcl[:, d_features]\n",
    "    yf = dcl[:, target_f]\n",
    "    with dpctl.device_context(\"opencl:gpu\"):\n",
    "        with parallel_backend(\"multiprocessing\"):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(xf, yf, test_size=0.5, random_state=0)\n",
    "            # mod = xrb(\n",
    "            #     n_estimators=100,\n",
    "            #     learning_rate=0.001,\n",
    "            #     booster=\"gbtree\",\n",
    "            #     n_jobs=-1,\n",
    "            #     gamma=0.00001,\n",
    "            #     random_state=0,\n",
    "            #     importance_type=\"total_gain\",\n",
    "            #     num_parallel_tree=10,\n",
    "            #     tree_method=\"approx\",\n",
    "            # )\n",
    "            mod = HistGradientBoostingRegressor(max_leaf_nodes=900)\n",
    "            work = make_pipeline(StandardScaler(), mod)\n",
    "            y_pred = work.fit(X_train, y_train).predict(X_test)\n",
    "            print(mean_squared_error(y_pred, y_test, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}