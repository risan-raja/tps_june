{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba63fcb-7dee-4ab0-9740-c05ff11dab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from joblib import parallel_backend\n",
    "from sklearn.linear_model import (\n",
    "    ARDRegression,\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    GammaRegressor,\n",
    "    Lasso,\n",
    "    LassoLars,\n",
    "    LassoLarsIC,\n",
    "    LinearRegression,\n",
    "    PassiveAggressiveRegressor,\n",
    "    PoissonRegressor,\n",
    "    QuantileRegressor,\n",
    "    RANSACRegressor,\n",
    "    SGDRegressor,\n",
    "    TheilSenRegressor,\n",
    "    TweedieRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearnex import patch_sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def conn():\n",
    "    with open(\"data_np.pkl\", \"rb\") as fp:\n",
    "        dpkl = pickle.load(fp)\n",
    "    return dpkl\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# cdef np.ndarray X__ = conn()\n",
    "X__ = conn()\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "def data_n_miss(n):\n",
    "    # data with x features simultaneously missing\n",
    "    # cdef np.ndarray t, idx\n",
    "    t = X__[:, 80]\n",
    "    idx = np.where(t == n)[0]\n",
    "    return X__[idx, :80]\n",
    "\n",
    "\n",
    "ms = data_n_miss(1)\n",
    "dcl = data_n_miss(0)\n",
    "\n",
    "iter_ = 10000\n",
    "tol = 0.000001\n",
    "\n",
    "estimators = [\n",
    "    LassoLarsIC(normalize=False, precompute=True, criterion=\"bic\"),\n",
    "    # LinearRegression(),\n",
    "    # SGDRegressor(learning_rate='adaptive'),\n",
    "    # ARDRegression(n_iter=1000,compute_score=True,tol=tol),\n",
    "    # BayesianRidge(lambda_init=0.001,n_iter=iter_,tol=tol,compute_score=True),\n",
    "    # PassiveAggressiveRegressor(C=0.5, max_iter=iter_,tol=tol,early_stopping=True,validation_fraction=0.3,n_iter_no_change=20),\n",
    "    # LinearSVR(tol=tol,max_iter=iter_,random_state=0,C=0.5),\n",
    "    # NuSVR(kernel='linear',tol=tol),\n",
    "    SVR(kernel=\"sigmoid\", tol=tol),\n",
    "    # Lasso(precompute=True,max_iter=iter_,tol=tol,selection='random'),\n",
    "    # LassoLars(precompbute=True,max_iter=iter_),RANSACRegressor(base_estimator=LassoLars(precompute=True,max_iter=iter_),max_trials=1000),TheilSenRegressor(n_jobs=-1),ElasticNet(precompute=True)\n",
    "]\n",
    "\n",
    "tweed_estimators = [\n",
    "    TweedieRegressor(power=0),\n",
    "    TweedieRegressor(power=1),\n",
    "    TweedieRegressor(power=1.5),\n",
    "    TweedieRegressor(power=2),\n",
    "    TweedieRegressor(power=3),\n",
    "]\n",
    "\n",
    "\n",
    "def feat_n_miss():\n",
    "    # Col Index of incomplete features\n",
    "    # cdef np.ndarray feed, feat_idx\n",
    "    global ms\n",
    "    feed: np.ndarray = np.isnan(ms).sum(axis=0)\n",
    "    feat_idx = np.where(feed > 0)[0]\n",
    "    return feat_idx\n",
    "\n",
    "\n",
    "def feat_x_nan_idx(feature, cc):\n",
    "    # cc data_n_miss\n",
    "    # return the indices of data missing \"x\" feature\n",
    "    cl_ = cc[:, feature]\n",
    "    return np.where(np.isnan(cl_))[0]\n",
    "\n",
    "\n",
    "# # clean data\n",
    "\n",
    "\n",
    "def get_clean_data(target_f, dirty_f_idx, size=0.5):\n",
    "    clash = np.array([target_f, dirty_f_idx])\n",
    "    d_features = np.setdiff1d(np.arange(80), clash)\n",
    "    test_f = dcl[:, d_features]\n",
    "    test_t = dcl[:, target_f]\n",
    "    xt, xst, yt, yst = train_test_split(test_f, test_t, test_size=size, random_state=0, shuffle=True)\n",
    "    return xt, yt\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "clean_results = defaultdict(dict)\n",
    "\n",
    "\n",
    "def clean_learning(target_f, size=0.7, exx=estimators):\n",
    "    global clean_results\n",
    "\n",
    "    d_features = np.setdiff1d(np.arange(80), np.array(target_f))\n",
    "    xf = dcl[:, d_features]\n",
    "    yf = dcl[:, target_f]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xf, yf, test_size=size, random_state=0, shuffle=True)\n",
    "\n",
    "    for e in tqdm(exx):\n",
    "        with parallel_backend(\"loky\"):\n",
    "            e.fit(X_train, y_train)\n",
    "            y_pred = e.predict(X_test)\n",
    "            safer = gc.collect()\n",
    "            clean_results[target_f][e.__class__.__name__] = mean_squared_error(y_pred, y_test)\n",
    "            safer = gc.collect()\n",
    "\n",
    "\n",
    "def dirty_df(target_f, dirty_f_idx, ms=ms):\n",
    "    # dirty_f_idx.append(target_f)\n",
    "    clash = np.array([target_f, dirty_f_idx])\n",
    "    # cdef np.ndarray clash\n",
    "    d_features = np.setdiff1d(np.arange(80), clash)\n",
    "    dirty_f_training_data_idx = feat_x_nan_idx(dirty_f_idx, ms)\n",
    "    ark = ms[dirty_f_training_data_idx, :]\n",
    "    ark = ark[:, d_features]\n",
    "    ark_target = ms[dirty_f_training_data_idx, target_f]\n",
    "    return ark, ark_target\n",
    "\n",
    "\n",
    "dirty_results = defaultdict(dict)\n",
    "dirty_tweed_results = defaultdict(dict)\n",
    "\n",
    "# # @jit(parallel=True)\n",
    "def roll(target_f, dirty_f_idx):\n",
    "    global estimators\n",
    "    dirty_results[target_f][dirty_f_idx] = {}\n",
    "    x_train, y_train = dirty_df(target_f, dirty_f_idx)\n",
    "    safer = gc.collect()\n",
    "    # workflow = make_pipeline((), estimator)\n",
    "    xtest, ytest = get_clean_data(target_f, dirty_f_idx)\n",
    "    for workflow in tqdm(estimators):\n",
    "        with parallel_backend(\"loky\", n_jobs=4):\n",
    "            print(\"=\" * 30 + f\"{workflow.__class__.__name__} started\" + \"=\" * 30)\n",
    "            workflow.fit(x_train, y_train)\n",
    "            y_pred = workflow.predict(xtest)\n",
    "            # safer = gc.collect()\n",
    "            dirty_results[target_f][dirty_f_idx][workflow.__class__.__name__] = mean_squared_error(y_pred, ytest)\n",
    "            # print(workflow.__class__.__name__,\":\",mean_squared_error(y_pred, ytest))\n",
    "\n",
    "\n",
    "# def roll_tweed(target_f,dirty_f_idx):\n",
    "#     global tweed_estimators\n",
    "#     dirty_tweed_results[target_f][dirty_f_idx]={}\n",
    "#     x_train, y_train = dirty_df(target_f,dirty_f_idx)\n",
    "#     # workflow = make_pipeline((), estimator)\n",
    "#     xtest, ytest = get_clean_data(target_f, dirty_f_idx)\n",
    "#     for i,workflow in tqdm(enumerate(tweed_estimators)):\n",
    "#         try:\n",
    "#             with parallel_backend('threading',n_jobs=-1):\n",
    "#                 workflow.fit(x_train, y_train)\n",
    "#                 y_pred = workflow.predict(xtest)\n",
    "#                 safer = gc.collect()\n",
    "#                 dirty_tweed_results[target_f][dirty_f_idx][workflow.__class__.__name__+str(i)] = mean_squared_error(y_pred, ytest)\n",
    "#         except:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c98228-d8f6-4afc-88ad-77e44ebee492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:10<00:00, 35.40s/it]\u001b[A\n",
      "  8%|███▍                                        | 1/13 [01:11<14:15, 71.27s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:09<00:00, 34.60s/it]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [02:20<12:53, 70.36s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:11<00:00, 35.60s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [03:32<07:18, 48.72s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:20<00:00, 40.13s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [04:53<07:48, 58.61s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:55<00:00, 57.79s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [06:49<08:53, 76.24s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:04<00:00, 62.44s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [08:54<09:07, 91.20s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:29<00:00, 74.72s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 8/13 [11:24<09:05, 109.02s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:13<00:00, 66.94s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 9/13 [13:39<07:46, 116.70s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:28<00:00, 74.18s/it]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 10/13 [16:08<06:19, 126.40s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:08<00:00, 64.28s/it]\u001b[A\n",
      " 85%|███████████████████████████████████▌      | 11/13 [18:17<04:14, 127.23s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:21<00:00, 40.85s/it]\u001b[A\n",
      " 92%|██████████████████████████████████████▊   | 12/13 [19:39<01:53, 113.71s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================LassoLarsIC started==============================\n",
      "==============================SVR started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [01:13<00:00, 36.92s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 13/13 [20:53<00:00, 96.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>0.997421</td>\n",
       "      <td>0.995632</td>\n",
       "      <td>0.995431</td>\n",
       "      <td>0.995730</td>\n",
       "      <td>0.995443</td>\n",
       "      <td>0.995454</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995987</td>\n",
       "      <td>0.995512</td>\n",
       "      <td>0.995416</td>\n",
       "      <td>0.995436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>1459.760117</td>\n",
       "      <td>1338.649096</td>\n",
       "      <td>1516.337904</td>\n",
       "      <td>1469.474201</td>\n",
       "      <td>1459.642579</td>\n",
       "      <td>1321.500459</td>\n",
       "      <td>1317.010702</td>\n",
       "      <td>1392.947091</td>\n",
       "      <td>1325.283934</td>\n",
       "      <td>1391.484880</td>\n",
       "      <td>1349.613733</td>\n",
       "      <td>1402.756681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0            1            3            4            5   \\\n",
       "LassoLarsIC     0.997421     0.995632     0.995431     0.995730     0.995443   \n",
       "SVR          1459.760117  1338.649096  1516.337904  1469.474201  1459.642579   \n",
       "\n",
       "                      6            7            8            9            10  \\\n",
       "LassoLarsIC     0.995454     0.995652     0.995414     0.995987     0.995512   \n",
       "SVR          1321.500459  1317.010702  1392.947091  1325.283934  1391.484880   \n",
       "\n",
       "                      11           12  \n",
       "LassoLarsIC     0.995416     0.995436  \n",
       "SVR          1349.613733  1402.756681  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = feat_n_miss()\n",
    "yu = 2\n",
    "for i in tqdm(g[:13]):\n",
    "    # clean_learning(i)\n",
    "    if i != yu:\n",
    "        roll(yu, i)\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dirty_results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07d71a-4d96-4e65-b3c3-24a8332557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import dpctl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    StackingRegressor,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    SelectPercentile,\n",
    "    SequentialFeatureSelector,\n",
    "    mutual_info_regression,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    ARDRegression,\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    GammaRegressor,\n",
    "    Lasso,\n",
    "    LassoLars,\n",
    "    LassoLarsIC,\n",
    "    LinearRegression,\n",
    "    MultiTaskElasticNet,\n",
    "    MultiTaskLasso,\n",
    "    PassiveAggressiveRegressor,\n",
    "    PoissonRegressor,\n",
    "    QuantileRegressor,\n",
    "    RANSACRegressor,\n",
    "    RidgeCV,\n",
    "    SGDRegressor,\n",
    "    TheilSenRegressor,\n",
    "    TweedieRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, PowerTransformer, QuantileTransformer, StandardScaler\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearnex.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRFRegressor as xrb\n",
    "\n",
    "pd.options.display.max_columns = 90\n",
    "pd.options.display.max_rows = 90\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_config(display=\"diagram\")\n",
    "dirty_results = defaultdict(dict)\n",
    "reference_metadata = defaultdict(dict)\n",
    "clean_results = defaultdict(dict)\n",
    "\n",
    "\n",
    "patch_sklearn()\n",
    "iter_ = 10000\n",
    "tol = 0.000001\n",
    "\n",
    "\n",
    "def gen_stack():\n",
    "    # Category Selector\n",
    "    cat_selector = make_column_selector(dtype_exclude=np.float32)\n",
    "    # Number Selector\n",
    "    numerical_selector = make_column_selector(dtype_exclude=np.uint8)\n",
    "    # category_transformer =\n",
    "    # Feature Selector\n",
    "    sel = SelectFromModel(estimator=ElasticNet(precompute=True), threshold=\"median\")\n",
    "    numeric_scaler = StandardScaler()\n",
    "    cat_scaler = OneHotEncoder(sparse=True)\n",
    "    linear_prep = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_scaler, numerical_selector), (\"categ\", cat_scaler, cat_selector)]\n",
    "    )\n",
    "    tree_prep = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_scaler, numerical_selector), (\"categ\", OrdinalEncoder, cat_selector)]\n",
    "    )\n",
    "    lasso_linear_prep = ColumnTransformer(transformers=[(\"num\", numeric_scaler, numerical_selector)])\n",
    "    modis = [\n",
    "        make_pipeline(lasso_linear_prep, sel, LassoLarsIC(normalize=False, precompute=True, criterion=\"bic\")),\n",
    "        make_pipeline(lasso_linear_prep, sel, ARDRegression(n_iter=1000, compute_score=True, tol=tol)),\n",
    "        make_pipeline(linear_prep, sel, BayesianRidge(lambda_init=0.001, n_iter=iter_, tol=tol, compute_score=True)),\n",
    "        make_pipeline(linear_prep, sel, Lasso(precompute=True, max_iter=iter_, tol=tol, selection=\"cyclic\")),\n",
    "        make_pipeline(linear_prep, sel, LassoLars(precompute=True, max_iter=iter_)),\n",
    "        make_pipeline(linear_prep, sel, TweedieRegressor(power=0)),\n",
    "        make_pipeline(\n",
    "            linear_prep,\n",
    "            sel,\n",
    "            RANSACRegressor(\n",
    "                min_samples=500,\n",
    "                base_estimator=LassoLarsIC(normalize=False, precompute=True, criterion=\"aic\"),\n",
    "                max_trials=10000,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            linear_prep,\n",
    "            sel,\n",
    "            ElasticNet(\n",
    "                precompute=True,\n",
    "            ),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, HistGradientBoostingRegressor(max_iter=1000, max_depth=500)),\n",
    "        make_pipeline(tree_prep, sel, GradientBoostingRegressor(random_state=0, max_depth=30)),\n",
    "        make_pipeline(tree_prep, sel, DecisionTreeRegressor()),\n",
    "        make_pipeline(\n",
    "            tree_prep,\n",
    "            sel,\n",
    "            ExtraTreesRegressor(n_jobs=-1),\n",
    "        ),\n",
    "        make_pipeline(tree_prep, sel, AdaBoostRegressor(base_estimator=Lasso(precompute=True))),\n",
    "    ]\n",
    "    stacked_estimators = []\n",
    "    for q in modis:\n",
    "        ename = q[2].__class__.__name__\n",
    "        stacked_estimators.append((ename, q))\n",
    "    learning_stack = StackingRegressor(estimators=stacked_estimators, cv=3, n_jobs=-1, final_estimator=RidgeCV())\n",
    "    return learning_stack\n",
    "\n",
    "\n",
    "with open(\"data.pkl\", \"rb\") as fp:\n",
    "    df = pickle.load(fp)\n",
    "\n",
    "trd = df[df.missing_cols == 0].copy()\n",
    "X_y = trd.drop([\"missing_cols\"], axis=1)\n",
    "\n",
    "\n",
    "def save_pipeline(c, p):\n",
    "    with open(f\"stacking_models/stack_{c}.pkl\", \"wb+\") as fp:\n",
    "        pickle.dump(p, fp)\n",
    "\n",
    "\n",
    "def get_data_feed(c):\n",
    "    X = X_y.drop([c], axis=1)\n",
    "    y = X_y[c]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# if\n",
    "#\n",
    "trgs = [\n",
    "    \"F_1_0\",\n",
    "    \"F_1_1\",\n",
    "    \"F_1_2\",\n",
    "    \"F_1_3\",\n",
    "    \"F_1_4\",\n",
    "    \"F_1_5\",\n",
    "    \"F_1_6\",\n",
    "    \"F_1_7\",\n",
    "    \"F_1_8\",\n",
    "    \"F_1_9\",\n",
    "    \"F_1_10\",\n",
    "    \"F_1_11\",\n",
    "    \"F_1_12\",\n",
    "    \"F_1_13\",\n",
    "    \"F_1_14\",\n",
    "    \"F_3_0\",\n",
    "    \"F_3_1\",\n",
    "    \"F_3_2\",\n",
    "    \"F_3_3\",\n",
    "    \"F_3_4\",\n",
    "    \"F_3_5\",\n",
    "    \"F_3_6\",\n",
    "    \"F_3_7\",\n",
    "    \"F_3_8\",\n",
    "    \"F_3_9\",\n",
    "    \"F_3_10\",\n",
    "    \"F_3_11\",\n",
    "    \"F_3_12\",\n",
    "    \"F_3_13\",\n",
    "    \"F_3_14\",\n",
    "    \"F_3_15\",\n",
    "    \"F_3_16\",\n",
    "    \"F_3_17\",\n",
    "    \"F_3_18\",\n",
    "    \"F_3_19\",\n",
    "    \"F_3_20\",\n",
    "    \"F_3_21\",\n",
    "    \"F_3_22\",\n",
    "    \"F_3_23\",\n",
    "    \"F_3_24\",\n",
    "    \"F_4_0\",\n",
    "    \"F_4_1\",\n",
    "    \"F_4_2\",\n",
    "    \"F_4_3\",\n",
    "    \"F_4_4\",\n",
    "    \"F_4_5\",\n",
    "    \"F_4_6\",\n",
    "    \"F_4_7\",\n",
    "    \"F_4_8\",\n",
    "    \"F_4_9\",\n",
    "    \"F_4_10\",\n",
    "    \"F_4_11\",\n",
    "    \"F_4_12\",\n",
    "    \"F_4_13\",\n",
    "    \"F_4_14\",\n",
    "]\n",
    "\n",
    "start = 3\n",
    "if start == 3:\n",
    "    for cl in trgs:\n",
    "        # with dpctl.device_context(\"opencl:gpu\"):\n",
    "        with parallel_backend(\"threading\", n_jobs=1):\n",
    "            gc.collect()\n",
    "            X_train, X_test, y_train, y_test = get_data_feed(cl)\n",
    "            new_stack = gen_stack()\n",
    "            gc.collect()\n",
    "\n",
    "            new_stack.fit(X_train, y_train)\n",
    "            yp = new_stack.predict(X_test)\n",
    "            save_pipeline(cl, new_stack)\n",
    "            print(mean_squared_error(yp, y_test))\n",
    "        break\n",
    "\n",
    "\n",
    "#\n",
    "# \tLassoLarsIC \tBayesianRidge \tLasso \tLassoLars \tTweedieRegressor \tRANSACRegressor \tElasticNet\n",
    "# 0 \t1.003801   \t1.003801 \t1.003801 \t1.003801 \t1.003801 \t1.004282 \t1.003801\n",
    "# 1 \t1.000459 \t1.000494 \t1.000459 \t1.000459 \t1.000464 \t1.002849 \t1.000459\n",
    "# 2 \t0.998591 \t0.998694 \t0.998591 \t0.998591 \t0.998607 \t0.999959 \t0.998591\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "target_f = 0\n",
    "if target_f == 9899:\n",
    "    d_features = np.setdiff1d(np.arange(80), np.array(target_f))\n",
    "    xf = dcl[:, d_features]\n",
    "    yf = dcl[:, target_f]\n",
    "    with dpctl.device_context(\"opencl:gpu\"):\n",
    "        with parallel_backend(\"multiprocessing\"):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(xf, yf, test_size=0.5, random_state=0)\n",
    "            # mod = xrb(\n",
    "            #     n_estimators=100,\n",
    "            #     learning_rate=0.001,\n",
    "            #     booster=\"gbtree\",\n",
    "            #     n_jobs=-1,\n",
    "            #     gamma=0.00001,\n",
    "            #     random_state=0,\n",
    "            #     importance_type=\"total_gain\",\n",
    "            #     num_parallel_tree=10,\n",
    "            #     tree_method=\"approx\",\n",
    "            # )\n",
    "            mod = HistGradientBoostingRegressor(max_leaf_nodes=900)\n",
    "            work = make_pipeline(StandardScaler(), mod)\n",
    "            y_pred = work.fit(X_train, y_train).predict(X_test)\n",
    "            print(mean_squared_error(y_pred, y_test, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
